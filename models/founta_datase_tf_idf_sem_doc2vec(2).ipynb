{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c0914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.60.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/martyna6525/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/martyna6525/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (4.0.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: textstat in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: pyphen in /opt/conda/lib/python3.7/site-packages (from textstat) (0.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import sys\n",
    "!pip install gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pip install textstat\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f9c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beats by Dr. Dre urBeats Wired In-Ear Headphon...</td>\n",
       "      <td>spam</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is time to draw close to Him &amp;#128591;&amp;#127...</td>\n",
       "      <td>normal</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you notice me start to act different or dis...</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forget unfollowers, I believe in growing. 7 ne...</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1  2\n",
       "0  Beats by Dr. Dre urBeats Wired In-Ear Headphon...     spam  4\n",
       "1  RT @Papapishu: Man it would fucking rule if we...  abusive  4\n",
       "2  It is time to draw close to Him &#128591;&#127...   normal  4\n",
       "3  if you notice me start to act different or dis...   normal  5\n",
       "4  Forget unfollowers, I believe in growing. 7 ne...   normal  3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('founta_dataset.csv', sep='\\\\t', header=None)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6924fcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beats by Dr. Dre urBeats Wired In-Ear Headphon...</td>\n",
       "      <td>spam</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is time to draw close to Him &amp;#128591;&amp;#127...</td>\n",
       "      <td>normal</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you notice me start to act different or dis...</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forget unfollowers, I believe in growing. 7 ne...</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text    label  votes\n",
       "ID                                                                   \n",
       "0   Beats by Dr. Dre urBeats Wired In-Ear Headphon...     spam      4\n",
       "1   RT @Papapishu: Man it would fucking rule if we...  abusive      4\n",
       "2   It is time to draw close to Him &#128591;&#127...   normal      4\n",
       "3   if you notice me start to act different or dis...   normal      5\n",
       "4   Forget unfollowers, I believe in growing. 7 ne...   normal      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.index.name = 'ID'\n",
    "dataset.columns = ['text', 'label', 'votes']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6828fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8656501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  \\\n",
      "ID                                                      \n",
      "0   Beats by Dr. Dre urBeats Wired In-Ear Headphon...   \n",
      "1   RT @Papapishu: Man it would fucking rule if we...   \n",
      "2   It is time to draw close to Him &#128591;&#127...   \n",
      "3   if you notice me start to act different or dis...   \n",
      "4   Forget unfollowers, I believe in growing. 7 ne...   \n",
      "5   RT @Vitiligoprince: Hate Being sexually Frustr...   \n",
      "6   Topped the group in TGP Disc Jam Season 2! Ont...   \n",
      "7   That daily baby aspirin for your #heart just m...   \n",
      "8   I liked a @YouTube video from @mattshea https:...   \n",
      "9   RT @LestuhGang_: If your fucking up &amp; your...   \n",
      "\n",
      "                                       processed_text  \n",
      "ID                                                     \n",
      "0          beat dr dre urbeat wire ear headphon white  \n",
      "1   papapishu man would fuck rule parti perpetu wa...  \n",
      "2              time draw close father draw near alway  \n",
      "3   notic start act differ distant bc peep someth ...  \n",
      "4   forget unfollow believ grow new follow last da...  \n",
      "5   vitiligoprinc hate sexual frustrat like wanna ...  \n",
      "6   top group tgp disc jam season onto semi final ...  \n",
      "7   daili babi aspirin heart might prevent colon c...  \n",
      "8   like youtub video mattshea blue armi come anci...  \n",
      "9        lestuhgang fuck amp homi dont tell fuck homi  \n"
     ]
    }
   ],
   "source": [
    "## 1. Removal of punctuation and capitlization\n",
    "## 2. Tokenizing\n",
    "## 3. Removal of stopwords\n",
    "## 4. Stemming\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):  \n",
    "    \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    text_space = text.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    text_name = text_space.str.replace(regex_pat, '')\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = text.str.replace(giant_url_regex, '')\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = text.str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # remove whitespace with a single space\n",
    "    newtext=punc_remove.str.replace(r'\\s+', ' ')\n",
    "    # remove leading and trailing whitespace\n",
    "    newtext=newtext.str.replace(r'^\\s+|\\s+?$','')\n",
    "    # replace normal numbers with numbr\n",
    "    newtext=newtext.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "    # removal of capitalization\n",
    "    text_lower = newtext.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_text = text_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_text=  tokenized_text.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the text\n",
    "    tokenized_text = tokenized_text.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "    \n",
    "    for i in range(len(tokenized_text)):\n",
    "        tokenized_text[i] = ' '.join(tokenized_text[i])\n",
    "        text_p= tokenized_text\n",
    "    \n",
    "    return text_p\n",
    "\n",
    "processed_text = preprocess(text)   \n",
    "\n",
    "dataset['processed_text'] = processed_text\n",
    "print(dataset[[\"text\",\"processed_text\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd919878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99996x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 863273 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Features-F1\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset['processed_text'] )\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ddbaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't specify the random_state in the code, \n",
    "# then every time you run(execute) your code a new random value is generated \n",
    "# and the train and test datasets would have different values each time.\n",
    "X = tfidf\n",
    "y = dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc20109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neu</th>\n",
       "      <th>Compound</th>\n",
       "      <th>url_tag</th>\n",
       "      <th>mention_tag</th>\n",
       "      <th>hash_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.116</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-0.1260</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.9430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99996 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Neg    Pos    Neu  Compound  url_tag  mention_tag  hash_tag\n",
       "0      0.000  0.000  1.000    0.0000      2.0          0.0       0.0\n",
       "1      0.116  0.143  0.741    0.1280      0.0          1.0       0.0\n",
       "2      0.000  0.000  1.000    0.0000      1.0          0.0       2.0\n",
       "3      0.000  0.000  1.000    0.0000      0.0          0.0       0.0\n",
       "4      0.138  0.107  0.755   -0.1260      1.0          0.0       0.0\n",
       "...      ...    ...    ...       ...      ...          ...       ...\n",
       "99991  0.000  0.000  1.000    0.0000      1.0          1.0       0.0\n",
       "99992  0.000  0.000  1.000    0.0000      2.0          0.0       3.0\n",
       "99993  0.405  0.000  0.595   -0.9430      0.0          1.0       4.0\n",
       "99994  0.000  0.000  1.000    0.0000      2.0          0.0       3.0\n",
       "99995  0.000  0.079  0.921    0.3612      0.0          0.0       0.0\n",
       "\n",
       "[99996 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer = VS()\n",
    "def count_tags(text_c):  \n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_c)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def sentiment_analysis(text):   \n",
    "    sentiment = sentiment_analyzer.polarity_scores(text)    \n",
    "    twitter_objs = count_tags(text)\n",
    "    features = [sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],twitter_objs[0], twitter_objs[1],\n",
    "                twitter_objs[2]]\n",
    "    #features = pd.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def sentiment_analysis_array(texts):\n",
    "    features=[]\n",
    "    for t in texts:\n",
    "        features.append(sentiment_analysis(t))\n",
    "    return np.array(features)\n",
    "\n",
    "final_features = sentiment_analysis_array(text)\n",
    "#final_features\n",
    "\n",
    "new_features = pd.DataFrame({'Neg':final_features[:,0],'Pos':final_features[:,1],'Neu':final_features[:,2],'Compound':final_features[:,3],\n",
    "                            'url_tag':final_features[:,4],'mention_tag':final_features[:,5],'hash_tag':final_features[:,6]})\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95a10977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99996, 10007)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F2-Conctaenation of tf-idf scores and sentiment scores\n",
    "tfidf_a = tfidf.toarray()\n",
    "modelling_features = np.concatenate([tfidf_a,final_features],axis=1)\n",
    "modelling_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88fbebb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc2vec_vector_0</th>\n",
       "      <th>doc2vec_vector_1</th>\n",
       "      <th>doc2vec_vector_2</th>\n",
       "      <th>doc2vec_vector_3</th>\n",
       "      <th>doc2vec_vector_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.007247</td>\n",
       "      <td>0.210878</td>\n",
       "      <td>0.245596</td>\n",
       "      <td>0.068894</td>\n",
       "      <td>-0.337243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.102374</td>\n",
       "      <td>0.277735</td>\n",
       "      <td>-0.035617</td>\n",
       "      <td>-0.053147</td>\n",
       "      <td>-0.115178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119710</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>-0.013994</td>\n",
       "      <td>-0.057815</td>\n",
       "      <td>0.006945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014502</td>\n",
       "      <td>-0.120260</td>\n",
       "      <td>0.227665</td>\n",
       "      <td>-0.037897</td>\n",
       "      <td>0.122347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.227692</td>\n",
       "      <td>-0.495088</td>\n",
       "      <td>0.224947</td>\n",
       "      <td>-0.215771</td>\n",
       "      <td>0.359873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>0.032149</td>\n",
       "      <td>0.181079</td>\n",
       "      <td>0.186056</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>-0.141475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>-0.061045</td>\n",
       "      <td>0.227174</td>\n",
       "      <td>0.222410</td>\n",
       "      <td>0.184262</td>\n",
       "      <td>-0.097945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>0.227166</td>\n",
       "      <td>-0.055357</td>\n",
       "      <td>-0.050826</td>\n",
       "      <td>0.183443</td>\n",
       "      <td>0.145716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>-0.194668</td>\n",
       "      <td>0.163657</td>\n",
       "      <td>-0.048632</td>\n",
       "      <td>-0.125261</td>\n",
       "      <td>0.004094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.057907</td>\n",
       "      <td>0.084408</td>\n",
       "      <td>0.076979</td>\n",
       "      <td>-0.128451</td>\n",
       "      <td>0.152491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99996 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc2vec_vector_0  doc2vec_vector_1  doc2vec_vector_2  doc2vec_vector_3  \\\n",
       "ID                                                                              \n",
       "0             -0.007247          0.210878          0.245596          0.068894   \n",
       "1              0.102374          0.277735         -0.035617         -0.053147   \n",
       "2              0.119710          0.002761         -0.013994         -0.057815   \n",
       "3              0.014502         -0.120260          0.227665         -0.037897   \n",
       "4             -0.227692         -0.495088          0.224947         -0.215771   \n",
       "...                 ...               ...               ...               ...   \n",
       "99991          0.032149          0.181079          0.186056          0.001743   \n",
       "99992         -0.061045          0.227174          0.222410          0.184262   \n",
       "99993          0.227166         -0.055357         -0.050826          0.183443   \n",
       "99994         -0.194668          0.163657         -0.048632         -0.125261   \n",
       "99995          0.057907          0.084408          0.076979         -0.128451   \n",
       "\n",
       "       doc2vec_vector_4  \n",
       "ID                       \n",
       "0             -0.337243  \n",
       "1             -0.115178  \n",
       "2              0.006945  \n",
       "3              0.122347  \n",
       "4              0.359873  \n",
       "...                 ...  \n",
       "99991         -0.141475  \n",
       "99992         -0.097945  \n",
       "99993          0.145716  \n",
       "99994          0.004094  \n",
       "99995          0.152491  \n",
       "\n",
       "[99996 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create doc2vec vector columns\n",
    "# Initialize and train the model\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#The input for a Doc2Vec model should be a list of TaggedDocument(['list','of','word'], [TAG_001]). \n",
    "#A good practice is using the indexes of sentences as the tags.\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset[\"processed_text\"].apply(lambda x: x.split(\" \")))]\n",
    "\n",
    "# train a Doc2Vec model with our text data\n",
    "# window- The maximum distance between the current and predicted word within a sentence.\n",
    "# mincount-Ignores all words with total frequency lower than this.\n",
    "# workers -Use these many worker threads to train the model\n",
    "#  Training Model - distributed bag of words (PV-DBOW) is employed.\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "#infer_vector - Infer a vector for given post-bulk training document.\n",
    "# Syntax- infer_vector(doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)\n",
    "# doc_words-A document for which the vector representation will be inferred.\n",
    "\n",
    "# transform each document into a vector data\n",
    "doc2vec_df = dataset[\"processed_text\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
    "doc2vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa6b5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99996, 10012)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conctaenation of tf-idf scores, sentiment scores and doc2vec columns\n",
    "modelling_features = np.concatenate([tfidf_a,final_features,doc2vec_df],axis=1)\n",
    "modelling_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447ba869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.90      0.88      5369\n",
      "     hateful       0.67      0.25      0.36       966\n",
      "      normal       0.81      0.92      0.86     10848\n",
      "        spam       0.62      0.41      0.49      2817\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.74      0.62      0.65     20000\n",
      "weighted avg       0.79      0.81      0.79     20000\n",
      "\n",
      "Logistic Regression, Accuracy Score: 0.80795\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(modelling_features)\n",
    "y = dataset['label']\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression().fit(X_train_bow,y_train)\n",
    "y_preds = model.predict(X_test_bow)\n",
    "report = classification_report(y_test, y_preds)\n",
    "print(report)\n",
    "acc = accuracy_score(y_test,y_preds)\n",
    "print(\"Logistic Regression, Accuracy Score:\" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a31c08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.85      0.91      0.88      5369\n",
      "     hateful       0.67      0.18      0.29       966\n",
      "      normal       0.79      0.94      0.86     10848\n",
      "        spam       0.66      0.23      0.34      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.74      0.56      0.59     20000\n",
      "weighted avg       0.78      0.80      0.76     20000\n",
      "\n",
      "Random Forest, Accuracy Score: 0.7955\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(modelling_features)\n",
    "y = dataset['label']\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_preds = rf.predict(X_test_bow)\n",
    "acc1 = accuracy_score(y_test,y_preds)\n",
    "report = classification_report(y_test, y_preds)\n",
    "print(report)\n",
    "print(\"Random Forest, Accuracy Score:\", acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee9285ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.61      0.36      0.45      5369\n",
      "     hateful       0.08      0.66      0.15       966\n",
      "      normal       0.81      0.19      0.30     10848\n",
      "        spam       0.29      0.67      0.40      2817\n",
      "\n",
      "    accuracy                           0.33     20000\n",
      "   macro avg       0.45      0.47      0.33     20000\n",
      "weighted avg       0.65      0.33      0.35     20000\n",
      "\n",
      "Naive Bayes, Accuracy Score: 0.32555\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(modelling_features)\n",
    "y = dataset['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_preds = nb.predict(X_test)\n",
    "acc2 = accuracy_score(y_test,y_preds)\n",
    "report = classification_report(y_test, y_preds)\n",
    "print(report)\n",
    "print(\"Naive Bayes, Accuracy Score:\", acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d2b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.90      0.88      5369\n",
      "     hateful       0.62      0.27      0.37       966\n",
      "      normal       0.81      0.91      0.86     10848\n",
      "        spam       0.59      0.41      0.49      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.72      0.62      0.65     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n",
      "SVM, Accuracy Score: 0.8035\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(modelling_features)\n",
    "y = dataset['label']\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "support = LinearSVC(random_state=20)\n",
    "support.fit(X_train_bow,y_train)\n",
    "y_preds = support.predict(X_test_bow)\n",
    "acc3 = accuracy_score(y_test,y_preds)\n",
    "report = classification_report(y_test, y_preds)\n",
    "print(report)\n",
    "print(\"SVM, Accuracy Score:\" , acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae73688f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = ('Logistic', 'RandomForest', 'Naive_bayes', 'SVM')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [acc,acc1,acc2,acc3]\n",
    "plt.bar(y_pos, performance, align = 'center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Algorithm Comparision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788dca5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
